import os, sys

import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from numpy.random import randint

#This tests the accuracy of neural networks run using a limited number of features working up from 1 to 600
#Each experiment is run 5 times with the accuracies averaged

#Takes the data from Feature Selection Experiments that has already been processed
df = pd.read_csv("../FeatureSelectionExperiments/data/exToptimized.csv")

Y = df.pop('labels')
Y = to_categorical(Y)

bigaccuracylist = list()
for i in range(1, 600):
    accuracylist = list()
    for j in range(5):
        smalldf = df.copy()

        #Randomly selects features to use and reduces smalldf to only having those features
        namelist = list()
        for k in range(i):
            name = df.columns[randint(len(df.columns))]
            while name in namelist:
                name = df.columns[randint(len(df.columns))]
            namelist.append(name)
        smalldf = smalldf.reindex(columns=namelist)

        X_train, X_test, Y_train, Y_test = train_test_split(smalldf, Y, test_size = 0.2)

        #layout model configuration
        model = Sequential()
        model.add(Dense(1024, activation="relu", input_dim = X_train.shape[1]))
        model.add(Dense(512, activation="relu"))
        model.add(Dense(200, activation="relu"))
        model.add(Dense(Y_train.shape[1], activation="softmax"))

        model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=['categorical_crossentropy','accuracy'])

        #Train the neural network then get results
        model.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=1, epochs=12, shuffle=True)
        predicted_valid_labels = np.argmax(model.predict(X_test), axis=1)
        valid_labels = np.argmax(Y_test, axis=1)

        #Calculate the accuracy of the testing set prediction
        test_range = len(valid_labels)
        sum = 0
        for k in range(0, test_range):
            if(predicted_valid_labels[k] == valid_labels[k]):
                sum = sum + 1
        accuracylist.append(sum / test_range)
    sum = 0
    for j in range(5):
        sum += accuracylist[j]
    bigaccuracylist.append(sum / 5)
    print("Number of Features = ", i, "\tAverage Accuracy = ", sum / 5)
print(bigaccuracylist)


#Output
#[0.237949052906597, 0.31685173089484, 0.3625081645983018, 0.4065316786414108, 0.47596342259960805, 0.5080339647289354, 0.5713912475506204, 0.5852384062704115, 0.6043762246897453, 0.6370346178967996, 0.6780535597648596, 0.6913781841933376, 0.7228608752449379, 0.7015676028739387, 0.7290659699542783, 0.7740692357935991, 0.7828216851730895, 0.7738079686479425, 0.8080339647289353, 0.7802743305029393, 0.801241018941868, 0.8094056172436316, 0.8250163291966036, 0.8516002612671457, 0.8301763553233181, 0.8417374265186153, 0.8325277596342261, 0.851143043762247, 0.8419986936642717, 0.8571521881123448, 0.867994774657087, 0.8627041149575441, 0.8463749183540168, 0.8691051600261266, 0.8642064010450685, 0.8757021554539518, 0.8797517962116264, 0.8755062050947094, 0.8733507511430438, 0.8890267798824298, 0.8913128674069235, 0.8803396472893533, 0.8847811887655126, 0.8961463096015676, 0.896995427824951, 0.8831482691051601, 0.8816459830176356, 0.8964728935336381, 0.8954278249510125, 0.8988896146309603, 0.9016329196603527, 0.8894186806009143, 0.9048334421946441, 0.9084911822338342, 0.905943827563684, 0.9100587851077726, 0.9003919007184846, 0.9106466361854997, 0.8996734160679294, 0.9026126714565643, 0.9136512083605487, 0.9033311561071196, 0.9135858915741345, 0.9128674069235794, 0.9052253429131287, 0.9087524493794905, 0.9175702155453951, 0.9132593076420639, 0.9182887001959503, 0.9197909862834749, 0.9169823644676682, 0.9051600261267145, 0.9202482037883737, 0.9140431090790333, 0.9248856956237754, 0.9239712606139777, 0.9219464402351404, 0.9292619203135206, 0.9242978445460484, 0.915676028739386, 0.92527759634226, 0.9194644023514043, 0.9165904637491836, 0.9204441541476159, 0.9231221423905943, 0.9276290006531678, 0.9160026126714564, 0.9218158066623122, 0.9204441541476159, 0.9290006531678643, 0.9281515349444808, 0.9235793598954931, 0.9286740692357937, 0.9262573481384717, 0.9271064663618549, 0.9280862181580666, 0.9273677335075113, 0.9189418680600914, 0.9195297191378184, 0.9235793598954933, 0.9303069888961464, 0.9224689745264533, 0.9295885042455911, 0.9249510124101894, 0.9340300457217505, 0.9316133246244285, 0.9286087524493795, 0.9311561071195296, 0.9256041802743304, 0.9358589157413455, 0.9352057478772045, 0.9214239059438276, 0.9265839320705421, 0.9333768778576094, 0.9331809274983671, 0.9359895493141737, 0.931939908556499, 0.9333115610711953, 0.9316786414108427, 0.9303069888961464, 0.9288047028086218, 0.9278902677988242, 0.9312214239059438, 0.9285434356629654, 0.9293272370999347, 0.9387328543435665, 0.9365120836054865, 0.9327890267798825, 0.9355976485956891, 0.9378837361201828, 0.9384062704114958, 0.9331156107119529, 0.9299150881776617, 0.9278249510124101, 0.9397126061397779, 0.9394513389941215, 0.9408883082952318, 0.9372305682560418, 0.9355976485956891, 0.93370346178968, 0.9419986936642717, 0.9353363814500326, 0.9408883082952318, 0.940300457217505, 0.9363161332462443, 0.9389288047028087, 0.9331809274983671, 0.9421946440235139, 0.9391900718484651, 0.9413455258001306, 0.9369039843239714, 0.9362508164598303, 0.938667537557152, 0.9361854996734161, 0.938667537557152, 0.9409536250816458, 0.9378184193337689, 0.9425212279555846, 0.9424559111691705, 0.9364467668190726, 0.9419333768778577, 0.9387328543435665, 0.9444807315480077, 0.9385369039843241, 0.9397126061397781, 0.940365774003919, 0.9355976485956891, 0.9406270411495754, 0.9495754408883084, 0.9345525800130634, 0.9451338994121489, 0.9378837361201828, 0.9424559111691705, 0.9338994121489224, 0.9355976485956891, 0.9427824951012409, 0.9459830176355322, 0.9416721097322013, 0.9413455258001304, 0.9400391900718483, 0.9419986936642717, 0.939124755062051, 0.9459177008491182, 0.9412148922273025, 0.9435662965382103, 0.9367080339647289, 0.9447419986936643, 0.9421293272370999, 0.9461136512083606, 0.9413455258001306, 0.9458523840627041, 0.9487916394513389, 0.9414108425865448, 0.9345525800130634, 0.9391900718484651, 0.940300457217505, 0.9474853037230568, 0.9466361854996734, 0.9412148922273025, 0.9454604833442195, 0.9429784454604834, 0.9339647289353363, 0.9496407576747223, 0.9329849771391248, 0.9489222730241671, 0.9489222730241671, 0.944023514043109, 0.9502939255388634, 0.9480078380143697, 0.9471587197909862, 0.94689745264533, 0.9421946440235139, 0.9413455258001306, 0.9468974526453298, 0.9458523840627041, 0.9477465708687133, 0.9521881123448728, 0.9456564337034618, 0.9443500979751797, 0.9480731548007839, 0.9478772044415414, 0.9466361854996734, 0.9431743958197257, 0.9427171783148269, 0.9395166557805356, 0.9479425212279555, 0.937295885042456, 0.9453298497713913, 0.9387328543435662, 0.9475506205094708, 0.9509470934030047, 0.9485956890920967, 0.9364467668190726, 0.943435662965382, 0.9531678641410842, 0.9323318092749837, 0.9495754408883081, 0.9374918354016982, 0.9451338994121489, 0.9436316133246244, 0.9436969301110384, 0.9487916394513389, 0.9462442847811887, 0.9424559111691705, 0.9483344219464402, 0.9440235140431092, 0.9480731548007839, 0.9512083605486609, 0.9493141737426518, 0.9460483344219464, 0.9478772044415414, 0.9489875898105813, 0.9439581972566948, 0.9479425212279556, 0.9442194644023513, 0.9471587197909862, 0.9425212279555846, 0.9413455258001306, 0.947550620509471, 0.9468321358589158, 0.9453951665578053, 0.9453951665578053, 0.9474199869366426, 0.9466361854996734, 0.9490529065969955, 0.9443500979751797, 0.9506858262573481, 0.9455258001306335, 0.9510777269758328, 0.9453951665578053, 0.9457870672762899, 0.9489875898105813, 0.9448073154800785, 0.9456564337034618, 0.9438928804702809, 0.946374918354017, 0.9474853037230568, 0.9472893533638146, 0.9389288047028087, 0.9478118876551275, 0.9455911169170477, 0.949967341606793, 0.9480731548007839, 0.9395819725669498, 0.9455911169170477, 0.9485303723056827, 0.9503592423252776, 0.9482037883736119, 0.9460483344219464, 0.9534291312867407, 0.9540822991508817, 0.9474853037230568, 0.9494448073154802, 0.9430437622468973, 0.9512736773350751, 0.9464402351404312, 0.9538863487916395, 0.9465708687132594, 0.9463096015676029, 0.9477465708687133, 0.9529065969954278, 0.9467668190725016, 0.9533638145003266, 0.9502939255388636, 0.9459177008491183, 0.9431743958197256, 0.9490529065969954, 0.940953625081646, 0.946374918354017, 0.9508817766165905, 0.9478118876551275, 0.9489222730241671, 0.9416721097322011, 0.9472240365774004, 0.9507511430437623, 0.9463096015676028, 0.9490529065969954, 0.9448073154800782, 0.9490529065969955, 0.9523187459177009, 0.9486610058785108, 0.9538863487916395, 0.9517962116263881, 0.9519921619856303, 0.9516655780535597, 0.9531025473546701, 0.9479425212279556, 0.9564337034617896, 0.9503592423252776, 0.9504245591116917, 0.9525146962769432, 0.9496407576747223, 0.9497713912475507, 0.9489222730241671, 0.9491835401698238, 0.9516655780535597, 0.9547354670150228, 0.9489875898105813, 0.9523840627041149, 0.9535597648595688, 0.9512083605486611, 0.9539516655780534, 0.9429784454604834, 0.9465055519268454, 0.9495101241018942, 0.9472893533638145, 0.9586544741998694, 0.9429784454604834, 0.9471587197909862, 0.950620509470934, 0.9532331809274982, 0.9534944480731546, 0.9538863487916395, 0.9455911169170477, 0.9463096015676028, 0.949902024820379, 0.9514043109079033, 0.9504898758981059, 0.9452645329849771, 0.9474853037230568, 0.9521881123448726, 0.9529065969954278, 0.9526453298497713, 0.947028086218158, 0.9525800130633574, 0.9512736773350751, 0.9515349444807317, 0.9533638145003266, 0.9399738732854344, 0.9438275636838668, 0.9465055519268452, 0.9549967341606793, 0.9534944480731549, 0.9483997387328543, 0.952971913781842, 0.9514696276943175, 0.9476812540822992, 0.953625081645983, 0.9513389941214893, 0.9546701502286087, 0.9548661005878512, 0.9507511430437623, 0.9518615284128021, 0.9497060744611364, 0.9507511430437623, 0.9549314173742651, 0.9517308948399739, 0.9472240365774004, 0.9544088830829525, 0.9531678641410842, 0.9444807315480078, 0.9559111691704768, 0.9509470934030047, 0.9504245591116917, 0.953037230568256, 0.9522534291312867, 0.9518615284128021, 0.9574787720444153, 0.9514043109079033, 0.9486610058785108, 0.9520574787720444, 0.9491182233834096, 0.9531025473546701, 0.9490529065969954, 0.9504245591116917, 0.9336381450032658, 0.9514696276943175, 0.9493141737426519, 0.9518615284128021, 0.9517308948399739, 0.953037230568256, 0.9452645329849771, 0.9478118876551275, 0.9526453298497713, 0.9525146962769432, 0.9571521881123448, 0.9513389941214893, 0.9579359895493142, 0.9525800130633574, 0.9514043109079033, 0.9557152188112346, 0.9504245591116917, 0.9534944480731549, 0.9581319399085565, 0.9527759634225996, 0.9585891574134552, 0.9506858262573481, 0.9524493794905291, 0.9522534291312867, 0.9478118876551275, 0.9508164598301765, 0.9568909209666885, 0.9435009797517961, 0.9569562377531025, 0.9512736773350751, 0.9412802090137167, 0.9561724363161332, 0.9496407576747223, 0.9581972566949707, 0.9489222730241671, 0.9577400391900719, 0.9477465708687133, 0.9544088830829522, 0.9583278902677987, 0.9553886348791639, 0.9556499020248204, 0.9502939255388634, 0.9557805355976488, 0.9498367080339648, 0.9531678641410843, 0.9512083605486609, 0.9547354670150229, 0.9549314173742651, 0.9412802090137167, 0.9579359895493141, 0.951796211626388, 0.9557805355976485, 0.9515349444807317, 0.9463096015676028, 0.9485303723056825, 0.9534291312867407, 0.9455258001306337, 0.9527106466361854, 0.9519268451992161, 0.9573481384715873, 0.9546701502286087, 0.9523840627041149, 0.9544741998693664, 0.956629653821032, 0.9559111691704768, 0.9542129327237099, 0.9542782495101241, 0.9472240365774004, 0.9531025473546701, 0.9531678641410842, 0.9515349444807315, 0.9499020248203788, 0.9509470934030044, 0.9576747224036577, 0.9535597648595688, 0.9478772044415414, 0.9389941214892227, 0.9537557152188112, 0.955453951665578, 0.946374918354017, 0.9520574787720444, 0.9436969301110384, 0.9574134552580013, 0.9523840627041149, 0.9554539516655781, 0.9553886348791639, 0.9529065969954278, 0.9549967341606793, 0.9486610058785108, 0.9488569562377531, 0.9549967341606793, 0.9522534291312867, 0.9522534291312867, 0.9531025473546701, 0.956694970607446, 0.9531025473546701, 0.9496407576747223, 0.9549314173742651, 0.9485303723056825, 0.9463096015676029, 0.9501632919660352, 0.9562377531025474, 0.953690398432397, 0.9479425212279555, 0.9541476159372959, 0.9525800130633574, 0.9474853037230568, 0.9529065969954278, 0.9551926845199216, 0.9582625734813848, 0.9555192684519922, 0.9527759634225996, 0.9521881123448726, 0.952971913781842, 0.9480078380143697, 0.952449379490529, 0.9591116917047682, 0.9563030698889614, 0.953037230568256, 0.959699542782495, 0.9497060744611364, 0.9551273677335075, 0.9545395166557806, 0.9572828216851731, 0.9534291312867407, 0.9515349444807315, 0.9528412802090138, 0.9538863487916395, 0.9340300457217505, 0.9501632919660352, 0.9495754408883084, 0.950620509470934, 0.9515349444807315, 0.9564990202482037, 0.950032658393207, 0.955976485956891, 0.9435009797517961, 0.9561724363161332, 0.9555845852384064, 0.9538863487916395, 0.9486610058785108, 0.9576094056172437, 0.9580666231221423, 0.9586544741998694, 0.9551926845199216, 0.9568909209666885, 0.9491182233834096, 0.9514043109079033, 0.9546701502286087, 0.9540822991508818, 0.9544741998693664, 0.9559111691704768, 0.9539516655780537, 0.9512736773350751, 0.9510777269758328, 0.9557805355976485, 0.9564990202482038, 0.9461789679947745, 0.9522534291312867, 0.9527759634225996, 0.9598954931417374, 0.9485303723056827, 0.955976485956891, 0.9516002612671457, 0.9531678641410842, 0.9551926845199217, 0.9548007838014371, 0.9546048334421947, 0.9546701502286087, 0.9475506205094708, 0.950620509470934, 0.9468974526453298, 0.9474853037230568, 0.9501632919660352, 0.956041802743305, 0.9544741998693664, 0.9547354670150229, 0.9577400391900719, 0.9547354670150229, 0.9539516655780534, 0.9603527106466363, 0.953625081645983, 0.9525146962769432, 0.9482037883736119, 0.9532984977139124, 0.9540169823644676, 0.9534291312867407, 0.9578053559764861, 0.9539516655780534, 0.9567602873938602, 0.9592423252775963, 0.9531025473546701, 0.9587851077726975, 0.9578053559764861]
